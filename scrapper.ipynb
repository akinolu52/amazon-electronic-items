{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.268716Z",
     "start_time": "2024-06-17T00:47:35.263691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import re\n",
    "from typing import Union, List, Dict\n",
    "from urllib.parse import unquote, quote_plus\n",
    "\n",
    "import pandas as pd\n",
    "# !pip install webdriver_manager"
   ],
   "id": "3338b9d47352218a",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.274475Z",
     "start_time": "2024-06-17T00:47:35.270808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ],
   "id": "4172a2f96c6e3891",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.301913Z",
     "start_time": "2024-06-17T00:47:35.297835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "electronic_household_items = [\n",
    "    \"television\", \"iron\", \"vacuum cleaner\", \"blender\", \"microwave\", \"toaster\", \"coffee maker\",\n",
    "    \"dishwasher\", \"refrigerator\", \"oven\", \"washer\", \"dryer\", \"air conditioner\", \"heater\", \"fan\",\n",
    "    \"hair dryer\", \"electric kettle\", \"rice cooker\", \"slow cooker\", \"pressure cooker\", \"food processor\",\n",
    "    \"mixer\", \"juicer\", \"grill\", \"waffle maker\", \"sandwich maker\", \"popcorn maker\", \"ice cream maker\",\n",
    "    \"electric griddle\", \"hot plate\", \"water purifier\", \"humidifier\", \"dehumidifier\", \"air purifier\",\n",
    "    \"ceiling fan\", \"space heater\", \"robot vacuum\", \"cordless drill\", \"security camera\", \"smart doorbell\",\n",
    "    \"smart lock\", \"thermostat\", \"smart light switch\", \"smart plug\", \"smart speaker\", \"sound bar\",\n",
    "    \"home theater system\", \"streaming device\", \"gaming console\", \"router\", \"smart TV\", \"wireless charger\",\n",
    "    \"Bluetooth speaker\", \"tablet\", \"e-reader\", \"smartphone\", \"smartwatch\", \"fitness tracker\", \"laptop\",\n",
    "    \"desktop computer\", \"monitor\", \"keyboard\", \"mouse\", \"external hard drive\", \"USB flash drive\",\n",
    "    \"headphones\", \"earbuds\", \"webcam\", \"digital camera\", \"camcorder\", \"drone\", \"smart light bulb\",\n",
    "    \"electric toothbrush\", \"electric shaver\", \"hair straightener\", \"hair curler\", \"foot massager\",\n",
    "    \"electric blanket\", \"electric skillet\", \"bread maker\", \"sous vide cooker\", \"food dehydrator\",\n",
    "    \"electric wine opener\", \"electric can opener\", \"electric knife\", \"electric pressure washer\",\n",
    "    \"smart refrigerator\", \"smart oven\", \"smart microwave\", \"robot lawn mower\", \"video doorbell\",\n",
    "    \"smart garage door opener\", \"smart blinds\", \"smart irrigation system\", \"smart smoke detector\",\n",
    "    \"smart carbon monoxide detector\", \"smart thermostat\", \"smart security system\", \"home automation hub\",\n",
    "    \"smart scale\", \"robot mop\", \"smart ceiling fan\", \"smart alarm clock\"\n",
    "]"
   ],
   "id": "9f6af491dd359ee5",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.305775Z",
     "start_time": "2024-06-17T00:47:35.302971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Product:\n",
    "    def __init__(self, product_id, name, category, price, description, about, brand, rating, image_url):\n",
    "        self.product_id = product_id\n",
    "        self.name = name\n",
    "        self.price = price\n",
    "        self.category = category\n",
    "        self.description = description\n",
    "        self.about = about\n",
    "        self.brand = brand\n",
    "        self.rating = rating\n",
    "        self.image_url = image_url\n",
    "        self.reviews = []\n",
    "\n",
    "    def add_review(self, review):\n",
    "        self.reviews.append(review)\n"
   ],
   "id": "bcc94eba1710fcf0",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.308998Z",
     "start_time": "2024-06-17T00:47:35.306833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Review:\n",
    "    def __init__(self, text: str):\n",
    "        self.text: str = text"
   ],
   "id": "ea5b9bfdeb572007",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:47:35.314151Z",
     "start_time": "2024-06-17T00:47:35.309923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewScrapper:\n",
    "    def __init__(self):\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    def fetch_reviews(self, product_link):\n",
    "        self.driver.get(product_link)\n",
    "        time.sleep(5)\n",
    "        reviews = []\n",
    "\n",
    "        try:\n",
    "            self.driver.find_element(By.PARTIAL_LINK_TEXT, \"See more reviews\").click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Fetch reviews from the next page\n",
    "            html_content = self.driver.page_source\n",
    "            reviews.extend(self.parse_review_page(html_content))\n",
    "\n",
    "            # Check for the next page link and click it\n",
    "            next_page = self.driver.find_elements(By.CSS_SELECTOR, \"li.a-last a\")\n",
    "            if next_page:\n",
    "                next_page[0].click()\n",
    "                time.sleep(5)  # Wait for the next page to load\n",
    "\n",
    "                # Fetch reviews from the next page\n",
    "                html_content = self.driver.page_source\n",
    "                reviews.extend(self.parse_review_page(html_content))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching reviews: {e}\")\n",
    "        finally:\n",
    "            self.cleanup()\n",
    "        return reviews\n",
    "\n",
    "    def parse_review_page(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        review_texts = [tag.get_text(strip=True) for tag in soup.find_all('span', {'data-hook': 'review-body'})]\n",
    "        return [Review(text) for text in review_texts]\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.driver.quit()\n"
   ],
   "id": "bcaba0673c5c8f55",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:48:24.594002Z",
     "start_time": "2024-06-17T00:47:35.315424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Scrapper:\n",
    "    def __init__(self):\n",
    "        self.keyword: Union[str | None] = None\n",
    "        self.product_count: int = 11\n",
    "        self.base_url: str = \"https://www.amazon.ca\"\n",
    "        self.products: List[Product] = []\n",
    "        self.skipped_categories: List[str] = []\n",
    "        self.max_retries: int = 1\n",
    "        self.categories: List[str] = electronic_household_items\n",
    "        # self.categories: List[str] = [\n",
    "        #     # \"smart TV\", 12-\n",
    "        #     # \"Bluetooth speaker\",\n",
    "        #     \"tablet\",\n",
    "        #     # \"e-reader\", \"smartphone\", \"smartwatch\", \"fitness tracker\", \"laptop\",\n",
    "        #     # \"desktop computer\", \"monitor\", \"keyboard\", \"mouse\", \"external hard drive\", \"USB flash drive\",\n",
    "        #     # \"smart plug\", \"smart speaker\", \"sound bar\",\n",
    "        # ]\n",
    "\n",
    "        self.headers: Dict[str, str] = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            'Accept-Language': 'en-US, en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "        }\n",
    "\n",
    "        # Initialize Selenium WebDriver\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    def set_keyword(self, keyword: str):\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def search_products(self):\n",
    "        search_url = f\"{self.base_url}/s?k={quote_plus(self.keyword)}\"\n",
    "\n",
    "        for _ in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.get(search_url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                print(search_url, response.status_code)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    product_links = [\n",
    "                        self.base_url + tag['href']\n",
    "                        for tag in soup.find_all('a', {'class': 'a-link-normal s-no-outline'}, href=True)\n",
    "                    ]\n",
    "\n",
    "                    print(f\"Found {len(product_links)} products\")\n",
    "                    return product_links[10:self.product_count]\n",
    "                else:\n",
    "                    print(f\"Failed to fetch the search results. Status code: {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Exception Error fetching search results: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching search results: {e}\")\n",
    "\n",
    "            # Wait before retrying\n",
    "            time.sleep(5)\n",
    "            # If all retries fail, add the keyword to skipped categories\n",
    "        self.skipped_categories.append(self.keyword)\n",
    "        return []\n",
    "\n",
    "    def extract_product_details(self, product_link: str):\n",
    "        response = requests.get(product_link, headers=self.headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                category = self.keyword\n",
    "\n",
    "                # Decode the URL to handle escaped characters\n",
    "                decoded_url = unquote(product_link)\n",
    "\n",
    "                # Regular expression pattern to match the ASIN in the URL\n",
    "                asin_pattern = r'/dp/([A-Z0-9]{10})/'\n",
    "                asin_match = re.search(asin_pattern, decoded_url)\n",
    "\n",
    "                # Return the matched ASIN or None if no match is found\n",
    "                product_id = asin_match.group(1) if asin_match else None\n",
    "\n",
    "                name = soup.find(id=\"productTitle\").get_text(strip=True)\n",
    "                name = name.strip() if name else None\n",
    "\n",
    "                brand = soup.select_one('tr.po-brand')\n",
    "                brand = brand.get_text(strip=True).replace('Brand', '') if brand else None\n",
    "\n",
    "                if brand is None:\n",
    "                    brand = soup.select_one('a#bylineInfo')\n",
    "                    brand = brand.get_text(strip=True).replace('Brand:', '') if brand else None\n",
    "\n",
    "                # scrape product price\n",
    "                price = soup.select_one('span.a-offscreen')\n",
    "                price = price.text.replace('$', '') if price else None\n",
    "\n",
    "                if not price:\n",
    "                    price_whole = soup.select_one('span.a-price-whole').get_text(strip=True)\n",
    "                    price_decimal = soup.select_one('span.a-price-fraction').get_text(strip=True)\n",
    "                    price = f'{price_whole}.{price_decimal}'\n",
    "\n",
    "                about = soup.find(id=\"feature-bullets\").get_text(strip=True)\n",
    "                about = about.strip() if about else None\n",
    "\n",
    "                description = soup.select_one('#productDescription')\n",
    "                description = description.text.strip() if description else None\n",
    "\n",
    "                image_url = soup.find(\"img\", {\"id\": \"landingImage\"})\n",
    "                image_url = image_url['src'] if image_url else None\n",
    "\n",
    "                # scrape product rating\n",
    "                rating_element = soup.select_one('#acrPopover').attrs.get('title')\n",
    "                rating = rating_element.replace('out of 5 stars', '') if rating_element else None\n",
    "\n",
    "                return Product(\n",
    "                    product_id=product_id,\n",
    "                    name=name,\n",
    "                    category=category,\n",
    "                    price=price,\n",
    "                    description=description,\n",
    "                    about=about,\n",
    "                    brand=brand,\n",
    "                    rating=rating,\n",
    "                    image_url=image_url\n",
    "                )\n",
    "            except AttributeError:\n",
    "                print(\"Error parsing product details.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Failed to fetch the product page.\")\n",
    "            return None\n",
    "\n",
    "    def aggregate_data(self):\n",
    "        product_links = self.search_products()\n",
    "\n",
    "        for link in product_links:\n",
    "            product = self.extract_product_details(link)\n",
    "\n",
    "            if product:\n",
    "                self.products.append(product)\n",
    "                review_scraper = ReviewScrapper()\n",
    "                reviews = review_scraper.fetch_reviews(link)\n",
    "\n",
    "                for review in reviews:\n",
    "                    product.add_review(review)\n",
    "\n",
    "    def save_to_csv(self, file_name: str):\n",
    "        data = []\n",
    "        columns = ['Id', 'Name', 'Category', 'Price', 'Description', 'About', 'Rating', 'ImageUrl', 'Brand', 'Review']\n",
    "\n",
    "        if len(self.products) == 0:\n",
    "            return\n",
    "\n",
    "        for product in self.products:\n",
    "            for review in product.reviews:\n",
    "                data.append({\n",
    "                    'Id': product.product_id,\n",
    "                    'Name': product.name,\n",
    "                    'Category': product.category,\n",
    "                    'Price': product.price,\n",
    "                    'Description': product.description,\n",
    "                    'About': product.about,\n",
    "                    'Rating': product.rating,\n",
    "                    'ImageUrl': product.image_url,\n",
    "                    'Brand': product.brand,\n",
    "                    'Review': review.text,\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.to_csv(file_name, index=False)\n",
    "\n",
    "        # reset the current data\n",
    "        self.products = []\n",
    "\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "\n",
    "    def merge_csv(self):\n",
    "        csv_files = glob.glob('./data/*.{}'.format('csv'))\n",
    "\n",
    "        if len(csv_files) == 0:\n",
    "            return\n",
    "\n",
    "        dataset = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "        dataset.to_csv('amazon_electronics_products_reviews.csv', index=False)\n",
    "\n",
    "        print(f\"Data saved and merged.\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def run(self):\n",
    "        for category in self.categories:\n",
    "            name = category.replace(' ', '_')\n",
    "            file_name = f'./data/amazon_{name}_products_reviews.csv'\n",
    "            self.set_keyword(category)\n",
    "            self.aggregate_data()\n",
    "            self.save_to_csv(file_name)\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        self.cleanup()\n",
    "        print(f'Skipped categories are: {self.skipped_categories}')\n",
    "        self.merge_csv()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scrapper()\n",
    "    scraper.run()\n"
   ],
   "id": "ddfdd57ab87f8204",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.ca/s?k=tablet 200\n",
      "Found 69 products\n",
      "Data saved to ./data/amazon_tablet_products_reviews.csv\n",
      "Skipped categories are: []\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T00:48:24.599241Z",
     "start_time": "2024-06-17T00:48:24.597031Z"
    }
   },
   "cell_type": "code",
   "source": " ",
   "id": "46cd8d0010c86c04",
   "outputs": [],
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
